{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score,roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 30473  30496  31002 ... 284804 284805 284806] Test: [    0     1     2 ... 57017 57018 57019]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [ 30473  30496  31002 ... 113964 113965 113966]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [ 81609  82400  83053 ... 170946 170947 170948]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [150654 150660 150661 ... 227866 227867 227868]\n",
      "Train: [     0      1      2 ... 227866 227867 227868] Test: [212516 212644 213092 ... 284804 284805 284806]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 227452, 1: 394})\n",
      "Counter({0: 56863, 1: 98})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(original_ytrain))\n",
    "print(Counter(original_ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ytrain = original_ytrain.values.reshape(-1,1)\n",
    "original_ytest = original_ytest.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = []\n",
    "column_indices.append(original_Xtrain.columns.get_loc('Time'))\n",
    "column_indices.append(original_Xtrain.columns.get_loc('Amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import RobustScaler \n",
    "column_trans = ColumnTransformer([('scaler', RobustScaler(), column_indices)], remainder='passthrough') \n",
    "column_trans.fit(original_Xtrain)\n",
    "original_Xtrain = column_trans.transform(original_Xtrain)\n",
    "original_Xtest = column_trans.transform(original_Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 200, 300, 400, 500]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt = SMOTE(sampling_strategy='minority')\n",
    "X_res, y_res = smt.fit_sample(original_Xtrain, original_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = list(range(500,1100,100))\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "log_reg_params = [{'penalty': ['l1','l2'], 'solver': ['liblinear'], 'max_iter': max_iter, 'C': C_values},\n",
    "              {'penalty': ['l2'], 'solver': ['lbfgs'], 'max_iter': max_iter , 'C': C_values}]\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "rand_log_reg.fit(X_res, y_res)\n",
    "best_est = rand_log_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, max_iter=600)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model._logistic.LogisticRegression"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(best_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "max_iter = list(range(500,1100,100))\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "log_reg_params = [{'penalty': ['l1','l2'], 'solver': ['liblinear'], 'max_iter': max_iter, 'C': C_values},\n",
    "              {'penalty': ['l2'], 'solver': ['lbfgs'], 'max_iter': max_iter , 'C': C_values}]\n",
    "\n",
    "# log_reg_params = {\"penalty\": ['l1', 'l2'],\"solver\": ['lbfgs', 'liblinear'], 'max_iter': list(range(100,600,100)) , 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "# rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg.best_estimator_)\n",
    "\n",
    "for train, test in sss.split(original_Xtrain, original_ytrain):   \n",
    "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "#     best_est = rand_log_reg.best_estimator_\n",
    "    prediction = model.predict(original_Xtrain[test])\n",
    "    \n",
    "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
    "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
    "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_est.predict(original_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = best_est.decision_function(original_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(original_ytest, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Fraud       1.00      0.99      0.99     56863\n",
      "       Fraud       0.11      0.86      0.20        98\n",
      "\n",
      "    accuracy                           0.99     56961\n",
      "   macro avg       0.56      0.92      0.60     56961\n",
      "weighted avg       1.00      0.99      0.99     56961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "smote_prediction = best_est.predict(original_Xtest)\n",
    "print(classification_report(original_ytest, smote_prediction, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
